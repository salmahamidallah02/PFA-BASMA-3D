{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13079061,"sourceType":"datasetVersion","datasetId":8283601}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-16T16:07:05.840989Z","iopub.execute_input":"2025-09-16T16:07:05.841277Z","iopub.status.idle":"2025-09-16T16:07:05.856601Z","shell.execute_reply.started":"2025-09-16T16:07:05.841253Z","shell.execute_reply":"2025-09-16T16:07:05.855996Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dataset/api_token_dict.csv\n/kaggle/input/dataset/dataset_test_nodup.csv\n/kaggle/input/dataset/dataset_train_nodup.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalAveragePooling1D, MultiHeadAttention, LayerNormalization\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# -----------------------------\n# Chemins Kaggle\n# -----------------------------\nDATA_DIR = \"/kaggle/input/dataset\"\nTRAIN_PATH = os.path.join(DATA_DIR, \"dataset_train_nodup.csv\")\nTEST_PATH = os.path.join(DATA_DIR, \"dataset_test_nodup.csv\")\nTOKEN_PATH = os.path.join(DATA_DIR, \"api_token_dict.csv\")\n\nMODEL_DIR = \"/kaggle/working/models\"\nos.makedirs(MODEL_DIR, exist_ok=True)\nMODEL_PATH = os.path.join(MODEL_DIR, \"lstm_attention_model.h5\")\nHIST_PATH = os.path.join(MODEL_DIR, \"training_history.png\")\nCM_PATH = os.path.join(MODEL_DIR, \"confusion_matrix.png\")\n\n# -----------------------------\n# VÃ©rification des fichiers\n# -----------------------------\nfor path in [TRAIN_PATH, TEST_PATH, TOKEN_PATH]:\n    if not os.path.exists(path):\n        print(f\"âŒ Fichier manquant : {path}\")\n        exit(1)\nprint(\"âœ… Tous les fichiers sont prÃ©sents.\")\n\n# -----------------------------\n# Lecture des donnÃ©es\n# -----------------------------\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\ntoken_df = pd.read_csv(TOKEN_PATH)\n\n# -----------------------------\n# PrÃ©paration des sÃ©quences\n# -----------------------------\nX_train = train_df['sequence_tokenized'].apply(eval).tolist()\nX_test = test_df['sequence_tokenized'].apply(eval).tolist()\n\nmaxlen = min(500, int(np.percentile([len(seq) for seq in X_train + X_test], 95)))\nprint(f\"Longueur max des sÃ©quences (maxlen) : {maxlen}\")\n\nX_train_pad = pad_sequences(X_train, maxlen=maxlen, padding='post', truncating='post')\nX_test_pad = pad_sequences(X_test, maxlen=maxlen, padding='post', truncating='post')\n\n# -----------------------------\n# PrÃ©paration des labels\n# -----------------------------\nlabels = sorted(train_df['label'].unique())\nlabel_dict = {label: idx for idx, label in enumerate(labels)}\ny_train = train_df['label'].map(label_dict).values\ny_test = test_df['label'].map(label_dict).values\ny_train_cat = to_categorical(y_train, num_classes=len(labels))\ny_test_cat = to_categorical(y_test, num_classes=len(labels))\n\n# -----------------------------\n# ParamÃ¨tres du modÃ¨le\n# -----------------------------\nvocab_size = len(token_df)\nembedding_dim = 256\nlstm_units = 256\n\n# -----------------------------\n# Architecture LSTM + Attention CORRIGÃ‰E\n# -----------------------------\ninputs = Input(shape=(maxlen,))\nembedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(inputs)\n\n# LSTM bidirectionnel\nlstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True))(embedding)\n\n# OPTION 1: Utiliser MultiHeadAttention (recommandÃ©)\nattention = MultiHeadAttention(\n    num_heads=8, \n    key_dim=64,\n    dropout=0.1\n)(lstm_out, lstm_out)\n\n# Connexion rÃ©siduelle et normalisation\nattention = LayerNormalization()(lstm_out + attention)\n\n# Pooling global avec masking\nglobal_pool = GlobalAveragePooling1D()(attention)\ndrop = Dropout(0.3)(global_pool)\noutput = Dense(len(labels), activation='softmax')(drop)\n\nmodel = Model(inputs, output)\nmodel.compile(\n    optimizer='adam', \n    loss='categorical_crossentropy', \n    metrics=['accuracy']\n)\n\nprint(model.summary())\n\n# -----------------------------\n# EntraÃ®nement avec callbacks\n# -----------------------------\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\n# Callbacks pour amÃ©liorer l'entraÃ®nement\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=3,\n    min_lr=0.00001\n)\n\nhistory = model.fit(\n    X_train_pad, y_train_cat,\n    validation_data=(X_test_pad, y_test_cat),\n    epochs=30,\n    batch_size=128,\n    callbacks=[early_stopping, reduce_lr],\n    verbose=1\n)\n\n# -----------------------------\n# Sauvegarde du modÃ¨le\n# -----------------------------\nmodel.save(MODEL_PATH)\nprint(f\"âœ… ModÃ¨le sauvegardÃ© : {MODEL_PATH}\")\n\n# Sauvegarde au format Keras natif\nMODEL_PATH_KERAS = MODEL_PATH.replace('.h5', '.keras')\nmodel.save(MODEL_PATH_KERAS)\nprint(f\"âœ… ModÃ¨le sauvegardÃ© (format Keras) : {MODEL_PATH_KERAS}\")\n\n# -----------------------------\n# Courbe d'apprentissage\n# -----------------------------\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train')\nplt.plot(history.history['val_accuracy'], label='Validation')\nplt.title('Courbe de prÃ©cision')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.title('Courbe de perte')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig(HIST_PATH)\nplt.close()\nprint(f\"âœ… Courbe de prÃ©cision sauvegardÃ©e : {HIST_PATH}\")\n\n# -----------------------------\n# PrÃ©dictions et matrice de confusion\n# -----------------------------\ny_pred = model.predict(X_test_pad)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test_cat, axis=1)\n\nprint(\"\\nRapport de classification :\")\nprint(classification_report(y_true, y_pred_classes, target_names=labels))\n\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.xlabel('PrÃ©diction')\nplt.ylabel('Vrai label')\nplt.title('Matrice de confusion')\nplt.tight_layout()\nplt.savefig(CM_PATH)\nplt.close()\nprint(f\"âœ… Matrice de confusion sauvegardÃ©e : {CM_PATH}\")\n\n# -----------------------------\n# Statistiques des sÃ©quences\n# -----------------------------\ntrain_lengths = [len(seq) for seq in X_train]\ntest_lengths = [len(seq) for seq in X_test]\nprint(f\"Longueur min/max/moyenne train : {min(train_lengths)}/{max(train_lengths)}/{np.mean(train_lengths):.1f}\")\nprint(f\"Longueur min/max/moyenne test : {min(test_lengths)}/{max(test_lengths)}/{np.mean(test_lengths):.1f}\")\n\nprint(\"\\nâœ… EntraÃ®nement terminÃ© avec succÃ¨s!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T17:04:28.222488Z","iopub.execute_input":"2025-09-16T17:04:28.223212Z","iopub.status.idle":"2025-09-16T17:19:12.077251Z","shell.execute_reply.started":"2025-09-16T17:04:28.223187Z","shell.execute_reply":"2025-09-16T17:19:12.076455Z"}},"outputs":[{"name":"stdout","text":"âœ… Tous les fichiers sont prÃ©sents.\nLongueur max des sÃ©quences (maxlen) : 500\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_7\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_5       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\nâ”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding_5         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m256\u001b[0m)  â”‚    \u001b[38;5;34m137,728\u001b[0m â”‚ input_layer_5[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”‚ (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ not_equal_7         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ input_layer_5[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\nâ”‚ (\u001b[38;5;33mNotEqual\u001b[0m)          â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_5     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚  \u001b[38;5;34m1,050,624\u001b[0m â”‚ embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mBidirectional\u001b[0m)     â”‚                   â”‚            â”‚ not_equal_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ multi_head_attentiâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚  \u001b[38;5;34m1,050,624\u001b[0m â”‚ bidirectional_5[\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mMultiHeadAttentioâ€¦\u001b[0m â”‚                   â”‚            â”‚ bidirectional_5[\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                     â”‚                   â”‚            â”‚ not_equal_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ not_equal_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (\u001b[38;5;33mAdd\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ bidirectional_5[\u001b[38;5;34mâ€¦\u001b[0m â”‚\nâ”‚                     â”‚                   â”‚            â”‚ multi_head_attenâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\nâ”‚ (\u001b[38;5;33mLayerNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ layer_normalizatâ€¦ â”‚\nâ”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ global_average_pâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)         â”‚      \u001b[38;5;34m4,617\u001b[0m â”‚ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer_5       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ embedding_5         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">137,728</span> â”‚ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ not_equal_7         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_5     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> â”‚ embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     â”‚                   â”‚            â”‚ not_equal_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ multi_head_attentiâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> â”‚ bidirectional_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentioâ€¦</span> â”‚                   â”‚            â”‚ bidirectional_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                     â”‚                   â”‚            â”‚ not_equal_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\nâ”‚                     â”‚                   â”‚            â”‚ not_equal_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\nâ”‚                     â”‚                   â”‚            â”‚ multi_head_attenâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ layer_normalization â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ global_average_pooâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalizatâ€¦ â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ global_average_pâ€¦ â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,617</span> â”‚ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,244,617\u001b[0m (8.56 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,244,617</span> (8.56 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,244,617\u001b[0m (8.56 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,244,617</span> (8.56 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"None\nEpoch 1/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 564ms/step - accuracy: 0.2896 - loss: 2.2751 - val_accuracy: 0.4896 - val_loss: 1.5431 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 545ms/step - accuracy: 0.4373 - loss: 1.5995 - val_accuracy: 0.5188 - val_loss: 1.3902 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 543ms/step - accuracy: 0.4780 - loss: 1.4514 - val_accuracy: 0.5045 - val_loss: 1.4414 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 547ms/step - accuracy: 0.5035 - loss: 1.4054 - val_accuracy: 0.5475 - val_loss: 1.3472 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 544ms/step - accuracy: 0.5324 - loss: 1.2986 - val_accuracy: 0.5430 - val_loss: 1.3046 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 547ms/step - accuracy: 0.5749 - loss: 1.2039 - val_accuracy: 0.5880 - val_loss: 1.2766 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 546ms/step - accuracy: 0.5877 - loss: 1.1437 - val_accuracy: 0.5930 - val_loss: 1.2679 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 545ms/step - accuracy: 0.6179 - loss: 1.0789 - val_accuracy: 0.5969 - val_loss: 1.2341 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 545ms/step - accuracy: 0.6249 - loss: 1.0452 - val_accuracy: 0.6142 - val_loss: 1.2166 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 544ms/step - accuracy: 0.6256 - loss: 1.0328 - val_accuracy: 0.5579 - val_loss: 1.2612 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 542ms/step - accuracy: 0.6281 - loss: 1.0518 - val_accuracy: 0.5633 - val_loss: 1.2776 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 542ms/step - accuracy: 0.6258 - loss: 1.0643 - val_accuracy: 0.5623 - val_loss: 1.2375 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 542ms/step - accuracy: 0.6742 - loss: 0.8942 - val_accuracy: 0.6058 - val_loss: 1.2161 - learning_rate: 2.0000e-04\nEpoch 14/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 541ms/step - accuracy: 0.7060 - loss: 0.8209 - val_accuracy: 0.6088 - val_loss: 1.2552 - learning_rate: 2.0000e-04\nEpoch 15/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 541ms/step - accuracy: 0.7209 - loss: 0.7958 - val_accuracy: 0.6058 - val_loss: 1.3088 - learning_rate: 2.0000e-04\nEpoch 16/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 542ms/step - accuracy: 0.7275 - loss: 0.7901 - val_accuracy: 0.6266 - val_loss: 1.3071 - learning_rate: 2.0000e-04\nEpoch 17/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 541ms/step - accuracy: 0.7434 - loss: 0.7204 - val_accuracy: 0.6286 - val_loss: 1.3137 - learning_rate: 4.0000e-05\nEpoch 18/30\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 542ms/step - accuracy: 0.7668 - loss: 0.6909 - val_accuracy: 0.6236 - val_loss: 1.3502 - learning_rate: 4.0000e-05\nâœ… ModÃ¨le sauvegardÃ© : /kaggle/working/models/lstm_attention_model.h5\nâœ… ModÃ¨le sauvegardÃ© (format Keras) : /kaggle/working/models/lstm_attention_model.keras\nâœ… Courbe de prÃ©cision sauvegardÃ©e : /kaggle/working/models/training_history.png\n\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step\n\nRapport de classification :\n              precision    recall  f1-score   support\n\n      Adware       0.87      1.00      0.93       225\n    Backdoor       0.54      0.43      0.48       224\n  Downloader       0.66      0.54      0.59       225\n     Dropper       0.33      0.35      0.34       225\n     Spyware       0.29      0.44      0.35       225\n      Trojan       0.42      0.34      0.38       224\n       Virus       0.81      0.78      0.80       225\n       Worms       0.63      0.58      0.60       224\n      benign       1.00      1.00      1.00       225\n\n    accuracy                           0.61      2022\n   macro avg       0.62      0.61      0.61      2022\nweighted avg       0.62      0.61      0.61      2022\n\nâœ… Matrice de confusion sauvegardÃ©e : /kaggle/working/models/confusion_matrix.png\nLongueur min/max/moyenne train : 4/1764421/19167.9\nLongueur min/max/moyenne test : 5/1045863/17478.4\n\nâœ… EntraÃ®nement terminÃ© avec succÃ¨s!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nimport random\nfrom collections import defaultdict\nimport json\n\n# -----------------------------\n# Configuration des chemins\n# -----------------------------\nDATA_DIR = \"/kaggle/input/dataset\"\nMODEL_DIR = \"/kaggle/working/models\"\nTEST_PATH = os.path.join(DATA_DIR, \"dataset_test_nodup.csv\")\nTOKEN_PATH = os.path.join(DATA_DIR, \"api_token_dict.csv\")\nMODEL_PATH = os.path.join(MODEL_DIR, \"lstm_attention_model.keras\")\n\n# Alternative si le modÃ¨le .keras n'existe pas\nif not os.path.exists(MODEL_PATH):\n    MODEL_PATH = os.path.join(MODEL_DIR, \"lstm_attention_model.h5\")\n\n# -----------------------------\n# Classe pour tester les prÃ©dictions\n# -----------------------------\nclass APIClassTester:\n    def __init__(self, model_path, test_data_path, token_path, maxlen=500):\n        self.maxlen = maxlen\n        self.model = None\n        self.test_df = None\n        self.token_df = None\n        self.labels = None\n        self.label_dict = None\n        self.reverse_label_dict = None\n        \n        self.load_data(test_data_path, token_path)\n        self.load_model(model_path)\n        self.results = []\n        \n    def load_data(self, test_data_path, token_path):\n        \"\"\"Charge les donnÃ©es de test et les tokens\"\"\"\n        print(\"ğŸ“ Chargement des donnÃ©es...\")\n        self.test_df = pd.read_csv(test_data_path)\n        self.token_df = pd.read_csv(token_path)\n        \n        # PrÃ©parer les labels\n        self.labels = sorted(self.test_df['label'].unique())\n        self.label_dict = {label: idx for idx, label in enumerate(self.labels)}\n        self.reverse_label_dict = {idx: label for label, idx in self.label_dict.items()}\n        \n        print(f\"âœ… DonnÃ©es chargÃ©es: {len(self.test_df)} Ã©chantillons\")\n        print(f\"ğŸ“‹ Classes dÃ©tectÃ©es: {self.labels}\")\n        \n    def load_model(self, model_path):\n        \"\"\"Charge le modÃ¨le prÃ©-entraÃ®nÃ©\"\"\"\n        if os.path.exists(model_path):\n            print(f\"ğŸ¤– Chargement du modÃ¨le: {model_path}\")\n            self.model = load_model(model_path)\n            print(\"âœ… ModÃ¨le chargÃ© avec succÃ¨s!\")\n        else:\n            print(f\"âŒ ModÃ¨le introuvable: {model_path}\")\n            raise FileNotFoundError(f\"ModÃ¨le non trouvÃ©: {model_path}\")\n    \n    def preprocess_sequence(self, sequence_str):\n        \"\"\"PrÃ©process une sÃ©quence pour la prÃ©diction\"\"\"\n        try:\n            # Convertir la chaÃ®ne en liste\n            sequence = eval(sequence_str) if isinstance(sequence_str, str) else sequence_str\n            # Padding\n            padded = pad_sequences([sequence], maxlen=self.maxlen, padding='post', truncating='post')\n            return padded\n        except:\n            print(f\"âš ï¸ Erreur lors du preprocessing de: {sequence_str}\")\n            return None\n    \n    def predict_single(self, sequence_str):\n        \"\"\"Fait une prÃ©diction sur une sÃ©quence\"\"\"\n        processed_seq = self.preprocess_sequence(sequence_str)\n        if processed_seq is None:\n            return None, 0.0\n            \n        # PrÃ©diction\n        prediction = self.model.predict(processed_seq, verbose=0)\n        predicted_class_idx = np.argmax(prediction)\n        confidence = np.max(prediction)\n        \n        predicted_label = self.reverse_label_dict[predicted_class_idx]\n        return predicted_label, confidence\n    \n    def test_random_samples_per_class(self, samples_per_class=3):\n        \"\"\"Teste des Ã©chantillons alÃ©atoires pour chaque classe\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"ğŸ§ª TEST DES PRÃ‰DICTIONS PAR CLASSE\")\n        print(\"=\"*80)\n        \n        total_tests = 0\n        correct_predictions = 0\n        \n        for class_label in self.labels:\n            print(f\"\\nğŸ·ï¸  CLASSE: {class_label}\")\n            print(\"-\" * 60)\n            \n            # RÃ©cupÃ©rer les Ã©chantillons de cette classe\n            class_samples = self.test_df[self.test_df['label'] == class_label]\n            \n            if len(class_samples) == 0:\n                print(f\"âš ï¸ Aucun Ã©chantillon trouvÃ© pour la classe {class_label}\")\n                continue\n            \n            # SÃ©lectionner des Ã©chantillons alÃ©atoires\n            n_samples = min(samples_per_class, len(class_samples))\n            selected_samples = class_samples.sample(n=n_samples, random_state=42)\n            \n            class_correct = 0\n            \n            for idx, (_, row) in enumerate(selected_samples.iterrows(), 1):\n                sequence = row['sequence_tokenized']\n                true_label = row['label']\n                \n                # PrÃ©diction\n                predicted_label, confidence = self.predict_single(sequence)\n                \n                if predicted_label is None:\n                    print(f\"  {idx}. âŒ ERREUR DE PRÃ‰DICTION\")\n                    continue\n                \n                # VÃ©rifier si la prÃ©diction est correcte\n                is_correct = predicted_label == true_label\n                status_icon = \"âœ…\" if is_correct else \"âŒ\"\n                \n                print(f\"  {idx}. {status_icon} Vrai: {true_label} | PrÃ©dit: {predicted_label} | Confiance: {confidence:.3f}\")\n                \n                # Enregistrer les rÃ©sultats\n                self.results.append({\n                    'true_label': true_label,\n                    'predicted_label': predicted_label,\n                    'confidence': confidence,\n                    'is_correct': is_correct,\n                    'sequence_sample': str(sequence)[:100] + '...'  # Ã‰chantillon de sÃ©quence\n                })\n                \n                total_tests += 1\n                if is_correct:\n                    correct_predictions += 1\n                    class_correct += 1\n            \n            # Score pour cette classe\n            class_accuracy = (class_correct / n_samples) * 100 if n_samples > 0 else 0\n            print(f\"  ğŸ“Š Score classe {class_label}: {class_correct}/{n_samples} = {class_accuracy:.1f}%\")\n        \n        # Score global\n        overall_accuracy = (correct_predictions / total_tests) * 100 if total_tests > 0 else 0\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"ğŸ“Š RÃ‰SULTATS FINAUX\")\n        print(\"=\"*80)\n        print(f\"ğŸ¯ Score global: {correct_predictions}/{total_tests} = {overall_accuracy:.2f}%\")\n        print(f\"ğŸ“ˆ PrÃ©dictions correctes: {correct_predictions}\")\n        print(f\"âŒ PrÃ©dictions incorrectes: {total_tests - correct_predictions}\")\n        \n        return overall_accuracy, self.results\n    \n    def detailed_analysis(self):\n        \"\"\"Analyse dÃ©taillÃ©e des rÃ©sultats par classe\"\"\"\n        if not self.results:\n            print(\"âš ï¸ Aucun rÃ©sultat Ã  analyser\")\n            return\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"ğŸ“ˆ ANALYSE DÃ‰TAILLÃ‰E PAR CLASSE\")\n        print(\"=\"*80)\n        \n        # Grouper par classe rÃ©elle\n        class_stats = defaultdict(lambda: {'total': 0, 'correct': 0, 'confidences': []})\n        \n        for result in self.results:\n            true_label = result['true_label']\n            class_stats[true_label]['total'] += 1\n            class_stats[true_label]['confidences'].append(result['confidence'])\n            if result['is_correct']:\n                class_stats[true_label]['correct'] += 1\n        \n        # Afficher les statistiques\n        for class_label in sorted(class_stats.keys()):\n            stats = class_stats[class_label]\n            accuracy = (stats['correct'] / stats['total']) * 100\n            avg_confidence = np.mean(stats['confidences'])\n            \n            print(f\"ğŸ·ï¸  {class_label}:\")\n            print(f\"    PrÃ©cision: {stats['correct']}/{stats['total']} = {accuracy:.1f}%\")\n            print(f\"    Confiance moyenne: {avg_confidence:.3f}\")\n            print()\n    \n    def save_results(self, output_path=\"/kaggle/working/test_results.json\"):\n        \"\"\"Sauvegarde les rÃ©sultats au format JSON\"\"\"\n        if not self.results:\n            print(\"âš ï¸ Aucun rÃ©sultat Ã  sauvegarder\")\n            return\n        \n        # PrÃ©parer les donnÃ©es pour la sauvegarde\n        save_data = {\n            'metadata': {\n                'total_tests': len(self.results),\n                'correct_predictions': sum(1 for r in self.results if r['is_correct']),\n                'overall_accuracy': sum(1 for r in self.results if r['is_correct']) / len(self.results) * 100,\n                'classes_tested': list(set(r['true_label'] for r in self.results))\n            },\n            'results': self.results\n        }\n        \n        with open(output_path, 'w', encoding='utf-8') as f:\n            json.dump(save_data, f, indent=2, ensure_ascii=False)\n        \n        print(f\"ğŸ’¾ RÃ©sultats sauvegardÃ©s: {output_path}\")\n\n# -----------------------------\n# Script principal\n# -----------------------------\ndef main():\n    try:\n        # Initialiser le testeur\n        tester = APIClassTester(\n            model_path=MODEL_PATH,\n            test_data_path=TEST_PATH,\n            token_path=TOKEN_PATH,\n            maxlen=500\n        )\n        \n        # Tester 2-3 Ã©chantillons par classe\n        print(\"ğŸš€ Lancement des tests...\")\n        overall_accuracy, results = tester.test_random_samples_per_class(samples_per_class=3)\n        \n        # Analyse dÃ©taillÃ©e\n        tester.detailed_analysis()\n        \n        # Sauvegarder les rÃ©sultats\n        tester.save_results()\n        \n        print(f\"\\nğŸ‰ Tests terminÃ©s! Score final: {overall_accuracy:.2f}%\")\n        \n    except Exception as e:\n        print(f\"âŒ Erreur lors des tests: {e}\")\n        import traceback\n        traceback.print_exc()\n\n# -----------------------------\n# Fonction de test individuel (pour usage manuel)\n# -----------------------------\ndef test_individual_prediction(sequence_tokenized, true_label=\"unknown\"):\n    \"\"\"\n    Fonction pour tester une prÃ©diction individuelle\n    Usage: test_individual_prediction(\"[1, 2, 3, 4, ...]\", \"vraie_classe\")\n    \"\"\"\n    try:\n        tester = APIClassTester(MODEL_PATH, TEST_PATH, TOKEN_PATH)\n        predicted_label, confidence = tester.predict_single(sequence_tokenized)\n        \n        print(f\"ğŸ·ï¸  Vraie classe: {true_label}\")\n        print(f\"ğŸ¤– PrÃ©diction: {predicted_label}\")\n        print(f\"ğŸ“Š Confiance: {confidence:.3f}\")\n        print(f\"âœ… Correct: {'Oui' if predicted_label == true_label else 'Non'}\")\n        \n        return predicted_label, confidence\n        \n    except Exception as e:\n        print(f\"âŒ Erreur: {e}\")\n        return None, 0.0\n\n# ExÃ©cuter le script principal\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T17:26:40.751151Z","iopub.execute_input":"2025-09-16T17:26:40.751451Z","iopub.status.idle":"2025-09-16T17:26:48.008423Z","shell.execute_reply.started":"2025-09-16T17:26:40.751429Z","shell.execute_reply":"2025-09-16T17:26:48.007514Z"}},"outputs":[{"name":"stdout","text":"ğŸ“ Chargement des donnÃ©es...\nâœ… DonnÃ©es chargÃ©es: 2022 Ã©chantillons\nğŸ“‹ Classes dÃ©tectÃ©es: ['Adware', 'Backdoor', 'Downloader', 'Dropper', 'Spyware', 'Trojan', 'Virus', 'Worms', 'benign']\nğŸ¤– Chargement du modÃ¨le: /kaggle/working/models/lstm_attention_model.keras\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 21 variables whereas the saved optimizer has 40 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"name":"stdout","text":"âœ… ModÃ¨le chargÃ© avec succÃ¨s!\nğŸš€ Lancement des tests...\n\n================================================================================\nğŸ§ª TEST DES PRÃ‰DICTIONS PAR CLASSE\n================================================================================\n\nğŸ·ï¸  CLASSE: Adware\n------------------------------------------------------------\n  1. âœ… Vrai: Adware | PrÃ©dit: Adware | Confiance: 0.973\n  2. âœ… Vrai: Adware | PrÃ©dit: Adware | Confiance: 0.995\n  3. âœ… Vrai: Adware | PrÃ©dit: Adware | Confiance: 0.995\n  ğŸ“Š Score classe Adware: 3/3 = 100.0%\n\nğŸ·ï¸  CLASSE: Backdoor\n------------------------------------------------------------\n  1. âŒ Vrai: Backdoor | PrÃ©dit: Worms | Confiance: 0.653\n  2. âŒ Vrai: Backdoor | PrÃ©dit: Trojan | Confiance: 0.499\n  3. âŒ Vrai: Backdoor | PrÃ©dit: Worms | Confiance: 0.600\n  ğŸ“Š Score classe Backdoor: 0/3 = 0.0%\n\nğŸ·ï¸  CLASSE: Downloader\n------------------------------------------------------------\n  1. âœ… Vrai: Downloader | PrÃ©dit: Downloader | Confiance: 0.928\n  2. âŒ Vrai: Downloader | PrÃ©dit: Spyware | Confiance: 0.292\n  3. âœ… Vrai: Downloader | PrÃ©dit: Downloader | Confiance: 0.618\n  ğŸ“Š Score classe Downloader: 2/3 = 66.7%\n\nğŸ·ï¸  CLASSE: Dropper\n------------------------------------------------------------\n  1. âŒ Vrai: Dropper | PrÃ©dit: Spyware | Confiance: 0.772\n  2. âœ… Vrai: Dropper | PrÃ©dit: Dropper | Confiance: 0.552\n  3. âœ… Vrai: Dropper | PrÃ©dit: Dropper | Confiance: 0.919\n  ğŸ“Š Score classe Dropper: 2/3 = 66.7%\n\nğŸ·ï¸  CLASSE: Spyware\n------------------------------------------------------------\n  1. âœ… Vrai: Spyware | PrÃ©dit: Spyware | Confiance: 0.887\n  2. âŒ Vrai: Spyware | PrÃ©dit: Dropper | Confiance: 0.574\n  3. âŒ Vrai: Spyware | PrÃ©dit: Worms | Confiance: 0.409\n  ğŸ“Š Score classe Spyware: 1/3 = 33.3%\n\nğŸ·ï¸  CLASSE: Trojan\n------------------------------------------------------------\n  1. âœ… Vrai: Trojan | PrÃ©dit: Trojan | Confiance: 0.536\n  2. âŒ Vrai: Trojan | PrÃ©dit: Spyware | Confiance: 0.250\n  3. âŒ Vrai: Trojan | PrÃ©dit: Spyware | Confiance: 0.277\n  ğŸ“Š Score classe Trojan: 1/3 = 33.3%\n\nğŸ·ï¸  CLASSE: Virus\n------------------------------------------------------------\n  1. âŒ Vrai: Virus | PrÃ©dit: Backdoor | Confiance: 0.397\n  2. âœ… Vrai: Virus | PrÃ©dit: Virus | Confiance: 1.000\n  3. âŒ Vrai: Virus | PrÃ©dit: Backdoor | Confiance: 0.397\n  ğŸ“Š Score classe Virus: 1/3 = 33.3%\n\nğŸ·ï¸  CLASSE: Worms\n------------------------------------------------------------\n  1. âŒ Vrai: Worms | PrÃ©dit: Spyware | Confiance: 0.588\n  2. âŒ Vrai: Worms | PrÃ©dit: Adware | Confiance: 0.234\n  3. âœ… Vrai: Worms | PrÃ©dit: Worms | Confiance: 0.926\n  ğŸ“Š Score classe Worms: 1/3 = 33.3%\n\nğŸ·ï¸  CLASSE: benign\n------------------------------------------------------------\n  1. âœ… Vrai: benign | PrÃ©dit: benign | Confiance: 1.000\n  2. âœ… Vrai: benign | PrÃ©dit: benign | Confiance: 1.000\n  3. âœ… Vrai: benign | PrÃ©dit: benign | Confiance: 1.000\n  ğŸ“Š Score classe benign: 3/3 = 100.0%\n\n================================================================================\nğŸ“Š RÃ‰SULTATS FINAUX\n================================================================================\nğŸ¯ Score global: 14/27 = 51.85%\nğŸ“ˆ PrÃ©dictions correctes: 14\nâŒ PrÃ©dictions incorrectes: 13\n\n================================================================================\nğŸ“ˆ ANALYSE DÃ‰TAILLÃ‰E PAR CLASSE\n================================================================================\nğŸ·ï¸  Adware:\n    PrÃ©cision: 3/3 = 100.0%\n    Confiance moyenne: 0.988\n\nğŸ·ï¸  Backdoor:\n    PrÃ©cision: 0/3 = 0.0%\n    Confiance moyenne: 0.584\n\nğŸ·ï¸  Downloader:\n    PrÃ©cision: 2/3 = 66.7%\n    Confiance moyenne: 0.613\n\nğŸ·ï¸  Dropper:\n    PrÃ©cision: 2/3 = 66.7%\n    Confiance moyenne: 0.748\n\nğŸ·ï¸  Spyware:\n    PrÃ©cision: 1/3 = 33.3%\n    Confiance moyenne: 0.623\n\nğŸ·ï¸  Trojan:\n    PrÃ©cision: 1/3 = 33.3%\n    Confiance moyenne: 0.354\n\nğŸ·ï¸  Virus:\n    PrÃ©cision: 1/3 = 33.3%\n    Confiance moyenne: 0.598\n\nğŸ·ï¸  Worms:\n    PrÃ©cision: 1/3 = 33.3%\n    Confiance moyenne: 0.582\n\nğŸ·ï¸  benign:\n    PrÃ©cision: 3/3 = 100.0%\n    Confiance moyenne: 1.000\n\nâŒ Erreur lors des tests: Object of type float32 is not JSON serializable\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_36/3721957647.py\", line 239, in main\n    tester.save_results()\n  File \"/tmp/ipykernel_36/3721957647.py\", line 214, in save_results\n    json.dump(save_data, f, indent=2, ensure_ascii=False)\n  File \"/usr/lib/python3.11/json/__init__.py\", line 179, in dump\n    for chunk in iterable:\n  File \"/usr/lib/python3.11/json/encoder.py\", line 432, in _iterencode\n    yield from _iterencode_dict(o, _current_indent_level)\n  File \"/usr/lib/python3.11/json/encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.11/json/encoder.py\", line 326, in _iterencode_list\n    yield from chunks\n  File \"/usr/lib/python3.11/json/encoder.py\", line 406, in _iterencode_dict\n    yield from chunks\n  File \"/usr/lib/python3.11/json/encoder.py\", line 439, in _iterencode\n    o = _default(o)\n        ^^^^^^^^^^^\n  File \"/usr/lib/python3.11/json/encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type float32 is not JSON serializable\n","output_type":"stream"}],"execution_count":22}]}