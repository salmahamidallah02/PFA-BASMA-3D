{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8944197",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-16T17:34:54.139793Z",
     "iopub.status.busy": "2025-09-16T17:34:54.139565Z",
     "iopub.status.idle": "2025-09-16T17:34:55.629926Z",
     "shell.execute_reply": "2025-09-16T17:34:55.628823Z"
    },
    "papermill": {
     "duration": 1.495144,
     "end_time": "2025-09-16T17:34:55.631298",
     "exception": false,
     "start_time": "2025-09-16T17:34:54.136154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset/api_token_dict.csv\n",
      "/kaggle/input/dataset/dataset_test_nodup.csv\n",
      "/kaggle/input/dataset/dataset_train_nodup.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68086772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:34:55.636471Z",
     "iopub.status.busy": "2025-09-16T17:34:55.636177Z",
     "iopub.status.idle": "2025-09-16T17:50:17.141773Z",
     "shell.execute_reply": "2025-09-16T17:50:17.140792Z"
    },
    "papermill": {
     "duration": 921.509924,
     "end_time": "2025-09-16T17:50:17.143348",
     "exception": false,
     "start_time": "2025-09-16T17:34:55.633424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 17:34:57.359797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758044097.572680      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758044097.630101      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tous les fichiers sont prÃ©sents.\n",
      "Longueur max des sÃ©quences (maxlen) : 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758044372.502760      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1758044372.503498      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">137,728</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ not_equal           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> â”‚ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     â”‚                   â”‚            â”‚ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_head_attentiâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> â”‚ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentioâ€¦</span> â”‚                   â”‚            â”‚ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ multi_head_attenâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ global_average_pâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)         â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,617</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m256\u001b[0m)  â”‚    \u001b[38;5;34m137,728\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ not_equal           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mNotEqual\u001b[0m)          â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚  \u001b[38;5;34m1,050,624\u001b[0m â”‚ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  â”‚\n",
       "â”‚ (\u001b[38;5;33mBidirectional\u001b[0m)     â”‚                   â”‚            â”‚ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ multi_head_attentiâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚  \u001b[38;5;34m1,050,624\u001b[0m â”‚ bidirectional[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mMultiHeadAttentioâ€¦\u001b[0m â”‚                   â”‚            â”‚ bidirectional[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add (\u001b[38;5;33mAdd\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ bidirectional[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ multi_head_attenâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ layer_normalization â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m512\u001b[0m)  â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â”‚ (\u001b[38;5;33mLayerNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ layer_normalizatâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ global_average_pâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)         â”‚      \u001b[38;5;34m4,617\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,244,617</span> (8.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,244,617\u001b[0m (8.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,244,617</span> (8.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,244,617\u001b[0m (8.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758044382.513122      63 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 517ms/step - accuracy: 0.2985 - loss: 2.1896 - val_accuracy: 0.4654 - val_loss: 1.4928 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 532ms/step - accuracy: 0.4359 - loss: 1.5589 - val_accuracy: 0.5158 - val_loss: 1.4208 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 581ms/step - accuracy: 0.4743 - loss: 1.4815 - val_accuracy: 0.4955 - val_loss: 1.4085 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 596ms/step - accuracy: 0.5078 - loss: 1.3797 - val_accuracy: 0.5391 - val_loss: 1.3093 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 582ms/step - accuracy: 0.5338 - loss: 1.2925 - val_accuracy: 0.5880 - val_loss: 1.2735 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 588ms/step - accuracy: 0.5594 - loss: 1.2355 - val_accuracy: 0.5964 - val_loss: 1.2372 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 586ms/step - accuracy: 0.5901 - loss: 1.1547 - val_accuracy: 0.5702 - val_loss: 1.3337 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 586ms/step - accuracy: 0.6099 - loss: 1.0982 - val_accuracy: 0.5776 - val_loss: 1.2548 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 585ms/step - accuracy: 0.6193 - loss: 1.0746 - val_accuracy: 0.5925 - val_loss: 1.2285 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 587ms/step - accuracy: 0.6296 - loss: 1.0409 - val_accuracy: 0.5781 - val_loss: 1.2549 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 578ms/step - accuracy: 0.6295 - loss: 1.0353 - val_accuracy: 0.5712 - val_loss: 1.2480 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 582ms/step - accuracy: 0.6125 - loss: 1.0983 - val_accuracy: 0.6014 - val_loss: 1.2131 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 581ms/step - accuracy: 0.6441 - loss: 1.0124 - val_accuracy: 0.6034 - val_loss: 1.2468 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 583ms/step - accuracy: 0.6691 - loss: 0.9426 - val_accuracy: 0.5920 - val_loss: 1.2567 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 578ms/step - accuracy: 0.6759 - loss: 0.9268 - val_accuracy: 0.5707 - val_loss: 1.2683 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 583ms/step - accuracy: 0.7167 - loss: 0.8111 - val_accuracy: 0.6142 - val_loss: 1.2301 - learning_rate: 2.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 582ms/step - accuracy: 0.7637 - loss: 0.6799 - val_accuracy: 0.6217 - val_loss: 1.2359 - learning_rate: 2.0000e-04\n",
      "âœ… ModÃ¨le sauvegardÃ© : /kaggle/working/models/lstm_attention_model.h5\n",
      "âœ… ModÃ¨le sauvegardÃ© (format Keras) : /kaggle/working/models/lstm_attention_model.keras\n",
      "âœ… Courbe de prÃ©cision sauvegardÃ©e : /kaggle/working/models/training_history.png\n",
      "\u001b[1m64/64\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 63ms/step\n",
      "\n",
      "Rapport de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Adware       0.88      1.00      0.94       225\n",
      "    Backdoor       0.42      0.59      0.49       224\n",
      "  Downloader       0.59      0.57      0.58       225\n",
      "     Dropper       0.45      0.35      0.39       225\n",
      "     Spyware       0.29      0.26      0.27       225\n",
      "      Trojan       0.50      0.31      0.38       224\n",
      "       Virus       0.74      0.80      0.77       225\n",
      "       Worms       0.48      0.54      0.51       224\n",
      "      benign       0.98      1.00      0.99       225\n",
      "\n",
      "    accuracy                           0.60      2022\n",
      "   macro avg       0.59      0.60      0.59      2022\n",
      "weighted avg       0.59      0.60      0.59      2022\n",
      "\n",
      "âœ… Matrice de confusion sauvegardÃ©e : /kaggle/working/models/confusion_matrix.png\n",
      "Longueur min/max/moyenne train : 4/1764421/19167.9\n",
      "Longueur min/max/moyenne test : 5/1045863/17478.4\n",
      "\n",
      "âœ… EntraÃ®nement terminÃ© avec succÃ¨s!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalAveragePooling1D, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------------\n",
    "# Chemins Kaggle\n",
    "# -----------------------------\n",
    "DATA_DIR = \"/kaggle/input/dataset\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"dataset_train_nodup.csv\")\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"dataset_test_nodup.csv\")\n",
    "TOKEN_PATH = os.path.join(DATA_DIR, \"api_token_dict.csv\")\n",
    "\n",
    "MODEL_DIR = \"/kaggle/working/models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"lstm_attention_model.h5\")\n",
    "HIST_PATH = os.path.join(MODEL_DIR, \"training_history.png\")\n",
    "CM_PATH = os.path.join(MODEL_DIR, \"confusion_matrix.png\")\n",
    "\n",
    "# -----------------------------\n",
    "# VÃ©rification des fichiers\n",
    "# -----------------------------\n",
    "for path in [TRAIN_PATH, TEST_PATH, TOKEN_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"âŒ Fichier manquant : {path}\")\n",
    "        exit(1)\n",
    "print(\"âœ… Tous les fichiers sont prÃ©sents.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Lecture des donnÃ©es\n",
    "# -----------------------------\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "token_df = pd.read_csv(TOKEN_PATH)\n",
    "\n",
    "# -----------------------------\n",
    "# PrÃ©paration des sÃ©quences\n",
    "# -----------------------------\n",
    "X_train = train_df['sequence_tokenized'].apply(eval).tolist()\n",
    "X_test = test_df['sequence_tokenized'].apply(eval).tolist()\n",
    "\n",
    "maxlen = min(500, int(np.percentile([len(seq) for seq in X_train + X_test], 95)))\n",
    "print(f\"Longueur max des sÃ©quences (maxlen) : {maxlen}\")\n",
    "\n",
    "X_train_pad = pad_sequences(X_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "# -----------------------------\n",
    "# PrÃ©paration des labels\n",
    "# -----------------------------\n",
    "labels = sorted(train_df['label'].unique())\n",
    "label_dict = {label: idx for idx, label in enumerate(labels)}\n",
    "y_train = train_df['label'].map(label_dict).values\n",
    "y_test = test_df['label'].map(label_dict).values\n",
    "y_train_cat = to_categorical(y_train, num_classes=len(labels))\n",
    "y_test_cat = to_categorical(y_test, num_classes=len(labels))\n",
    "\n",
    "# -----------------------------\n",
    "# ParamÃ¨tres du modÃ¨le\n",
    "# -----------------------------\n",
    "vocab_size = len(token_df)\n",
    "embedding_dim = 256\n",
    "lstm_units = 256\n",
    "\n",
    "# -----------------------------\n",
    "# Architecture LSTM + Attention CORRIGÃ‰E\n",
    "# -----------------------------\n",
    "inputs = Input(shape=(maxlen,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(inputs)\n",
    "\n",
    "# LSTM bidirectionnel\n",
    "lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True))(embedding)\n",
    "\n",
    "# OPTION 1: Utiliser MultiHeadAttention (recommandÃ©)\n",
    "attention = MultiHeadAttention(\n",
    "    num_heads=8, \n",
    "    key_dim=64,\n",
    "    dropout=0.1\n",
    ")(lstm_out, lstm_out)\n",
    "\n",
    "# Connexion rÃ©siduelle et normalisation\n",
    "attention = LayerNormalization()(lstm_out + attention)\n",
    "\n",
    "# Pooling global avec masking\n",
    "global_pool = GlobalAveragePooling1D()(attention)\n",
    "drop = Dropout(0.3)(global_pool)\n",
    "output = Dense(len(labels), activation='softmax')(drop)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# -----------------------------\n",
    "# EntraÃ®nement avec callbacks\n",
    "# -----------------------------\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Callbacks pour amÃ©liorer l'entraÃ®nement\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train_cat,\n",
    "    validation_data=(X_test_pad, y_test_cat),\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Sauvegarde du modÃ¨le\n",
    "# -----------------------------\n",
    "model.save(MODEL_PATH)\n",
    "print(f\"âœ… ModÃ¨le sauvegardÃ© : {MODEL_PATH}\")\n",
    "\n",
    "# Sauvegarde au format Keras natif\n",
    "MODEL_PATH_KERAS = MODEL_PATH.replace('.h5', '.keras')\n",
    "model.save(MODEL_PATH_KERAS)\n",
    "print(f\"âœ… ModÃ¨le sauvegardÃ© (format Keras) : {MODEL_PATH_KERAS}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Courbe d'apprentissage\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Courbe de prÃ©cision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Courbe de perte')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(HIST_PATH)\n",
    "plt.close()\n",
    "print(f\"âœ… Courbe de prÃ©cision sauvegardÃ©e : {HIST_PATH}\")\n",
    "\n",
    "# -----------------------------\n",
    "# PrÃ©dictions et matrice de confusion\n",
    "# -----------------------------\n",
    "y_pred = model.predict(X_test_pad)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test_cat, axis=1)\n",
    "\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_true, y_pred_classes, target_names=labels))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('PrÃ©diction')\n",
    "plt.ylabel('Vrai label')\n",
    "plt.title('Matrice de confusion')\n",
    "plt.tight_layout()\n",
    "plt.savefig(CM_PATH)\n",
    "plt.close()\n",
    "print(f\"âœ… Matrice de confusion sauvegardÃ©e : {CM_PATH}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Statistiques des sÃ©quences\n",
    "# -----------------------------\n",
    "train_lengths = [len(seq) for seq in X_train]\n",
    "test_lengths = [len(seq) for seq in X_test]\n",
    "print(f\"Longueur min/max/moyenne train : {min(train_lengths)}/{max(train_lengths)}/{np.mean(train_lengths):.1f}\")\n",
    "print(f\"Longueur min/max/moyenne test : {min(test_lengths)}/{max(test_lengths)}/{np.mean(test_lengths):.1f}\")\n",
    "\n",
    "print(\"\\nâœ… EntraÃ®nement terminÃ© avec succÃ¨s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08252f1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T17:50:17.261252Z",
     "iopub.status.busy": "2025-09-16T17:50:17.260973Z",
     "iopub.status.idle": "2025-09-16T17:50:25.831273Z",
     "shell.execute_reply": "2025-09-16T17:50:25.830295Z"
    },
    "papermill": {
     "duration": 8.63036,
     "end_time": "2025-09-16T17:50:25.832560",
     "exception": false,
     "start_time": "2025-09-16T17:50:17.202200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Chargement des donnÃ©es...\n",
      "âœ… DonnÃ©es chargÃ©es: 2022 Ã©chantillons\n",
      "ğŸ“‹ Classes dÃ©tectÃ©es: ['Adware', 'Backdoor', 'Downloader', 'Dropper', 'Spyware', 'Trojan', 'Virus', 'Worms', 'benign']\n",
      "ğŸ¤– Chargement du modÃ¨le: /kaggle/working/models/lstm_attention_model.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 21 variables whereas the saved optimizer has 40 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ModÃ¨le chargÃ© avec succÃ¨s!\n",
      "ğŸš€ Lancement des tests...\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª TEST DES PRÃ‰DICTIONS PAR CLASSE\n",
      "================================================================================\n",
      "\n",
      "ğŸ·ï¸  CLASSE: Adware\n",
      "------------------------------------------------------------\n",
      "  1. âœ… Vrai: Adware | PrÃ©dit: Adware | Confiance: 0.989\n",
      "  2. âœ… Vrai: Adware | PrÃ©dit: Adware | Confiance: 0.281\n",
      "  3. âœ… Vrai: Adware | PrÃ©dit: Adware | Confiance: 0.281\n",
      "  ğŸ“Š Score classe Adware: 3/3 = 100.0%\n",
      "\n",
      "ğŸ·ï¸  CLASSE: Backdoor\n",
      "------------------------------------------------------------\n",
      "  1. âœ… Vrai: Backdoor | PrÃ©dit: Backdoor | Confiance: 0.760\n",
      "  2. âœ… Vrai: Backdoor | PrÃ©dit: Backdoor | Confiance: 0.415\n",
      "  3. âŒ Vrai: Backdoor | PrÃ©dit: Worms | Confiance: 0.536\n",
      "  ğŸ“Š Score classe Backdoor: 2/3 = 66.7%\n",
      "\n",
      "ğŸ·ï¸  CLASSE: Downloader\n",
      "------------------------------------------------------------\n",
      "  1. âœ… Vrai: Downloader | PrÃ©dit: Downloader | Confiance: 0.971\n",
      "  2. âŒ Vrai: Downloader | PrÃ©dit: Spyware | Confiance: 0.203\n",
      "  3. âœ… Vrai: Downloader | PrÃ©dit: Downloader | Confiance: 0.839\n",
      "  ğŸ“Š Score classe Downloader: 2/3 = 66.7%\n",
      "\n",
      "ğŸ·ï¸  CLASSE: Dropper\n",
      "------------------------------------------------------------\n",
      "  1. âŒ Vrai: Dropper | PrÃ©dit: Spyware | Confiance: 0.338\n",
      "  2. âŒ Vrai: Dropper | PrÃ©dit: Spyware | Confiance: 0.888\n",
      "  3. âœ… Vrai: Dropper | PrÃ©dit: Dropper | Confiance: 0.480\n",
      "  ğŸ“Š Score classe Dropper: 1/3 = 33.3%\n",
      "\n",
      "ğŸ·ï¸  CLASSE: Spyware\n",
      "------------------------------------------------------------\n",
      "  1. âœ… Vrai: Spyware | PrÃ©dit: Spyware | Confiance: 0.346\n",
      "  2. âŒ Vrai: Spyware | PrÃ©dit: Dropper | Confiance: 0.418\n",
      "  3. âœ… Vrai: Spyware | PrÃ©dit: Spyware | Confiance: 0.432\n",
      "  ğŸ“Š Score classe Spyware: 2/3 = 66.7%\n",
      "\n",
      "ğŸ·ï¸  CLASSE: Trojan\n",
      "------------------------------------------------------------\n",
      "  1. âœ… Vrai: Trojan | PrÃ©dit: Trojan | Confiance: 0.443\n",
      "  2. âœ… Vrai: Trojan | PrÃ©dit: Trojan | Confiance: 0.321\n",
      "  3. âŒ Vrai: Trojan | PrÃ©dit: Backdoor | Confiance: 0.213\n",
      "  ğŸ“Š Score classe Trojan: 2/3 = 66.7%\n",
      "\n",
      "ğŸ·ï¸  CLASSE: Virus\n",
      "------------------------------------------------------------\n",
      "  1. âŒ Vrai: Virus | PrÃ©dit: Worms | Confiance: 0.578\n",
      "  2. âœ… Vrai: Virus | PrÃ©dit: Virus | Confiance: 1.000\n",
      "  3. âŒ Vrai: Virus | PrÃ©dit: Worms | Confiance: 0.578\n",
      "  ğŸ“Š Score classe Virus: 1/3 = 33.3%\n",
      "\n",
      "ğŸ·ï¸  CLASSE: Worms\n",
      "------------------------------------------------------------\n",
      "  1. âŒ Vrai: Worms | PrÃ©dit: Trojan | Confiance: 0.552\n",
      "  2. âŒ Vrai: Worms | PrÃ©dit: Backdoor | Confiance: 0.290\n",
      "  3. âœ… Vrai: Worms | PrÃ©dit: Worms | Confiance: 0.967\n",
      "  ğŸ“Š Score classe Worms: 1/3 = 33.3%\n",
      "\n",
      "ğŸ·ï¸  CLASSE: benign\n",
      "------------------------------------------------------------\n",
      "  1. âœ… Vrai: benign | PrÃ©dit: benign | Confiance: 1.000\n",
      "  2. âœ… Vrai: benign | PrÃ©dit: benign | Confiance: 0.993\n",
      "  3. âœ… Vrai: benign | PrÃ©dit: benign | Confiance: 1.000\n",
      "  ğŸ“Š Score classe benign: 3/3 = 100.0%\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š RÃ‰SULTATS FINAUX\n",
      "================================================================================\n",
      "ğŸ¯ Score global: 17/27 = 62.96%\n",
      "ğŸ“ˆ PrÃ©dictions correctes: 17\n",
      "âŒ PrÃ©dictions incorrectes: 10\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ ANALYSE DÃ‰TAILLÃ‰E PAR CLASSE\n",
      "================================================================================\n",
      "ğŸ·ï¸  Adware:\n",
      "    PrÃ©cision: 3/3 = 100.0%\n",
      "    Confiance moyenne: 0.517\n",
      "\n",
      "ğŸ·ï¸  Backdoor:\n",
      "    PrÃ©cision: 2/3 = 66.7%\n",
      "    Confiance moyenne: 0.571\n",
      "\n",
      "ğŸ·ï¸  Downloader:\n",
      "    PrÃ©cision: 2/3 = 66.7%\n",
      "    Confiance moyenne: 0.671\n",
      "\n",
      "ğŸ·ï¸  Dropper:\n",
      "    PrÃ©cision: 1/3 = 33.3%\n",
      "    Confiance moyenne: 0.569\n",
      "\n",
      "ğŸ·ï¸  Spyware:\n",
      "    PrÃ©cision: 2/3 = 66.7%\n",
      "    Confiance moyenne: 0.399\n",
      "\n",
      "ğŸ·ï¸  Trojan:\n",
      "    PrÃ©cision: 2/3 = 66.7%\n",
      "    Confiance moyenne: 0.325\n",
      "\n",
      "ğŸ·ï¸  Virus:\n",
      "    PrÃ©cision: 1/3 = 33.3%\n",
      "    Confiance moyenne: 0.719\n",
      "\n",
      "ğŸ·ï¸  Worms:\n",
      "    PrÃ©cision: 1/3 = 33.3%\n",
      "    Confiance moyenne: 0.603\n",
      "\n",
      "ğŸ·ï¸  benign:\n",
      "    PrÃ©cision: 3/3 = 100.0%\n",
      "    Confiance moyenne: 0.998\n",
      "\n",
      "âŒ Erreur lors des tests: Object of type float32 is not JSON serializable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_19/3721957647.py\", line 239, in main\n",
      "    tester.save_results()\n",
      "  File \"/tmp/ipykernel_19/3721957647.py\", line 214, in save_results\n",
      "    json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
      "  File \"/usr/lib/python3.11/json/__init__.py\", line 179, in dump\n",
      "    for chunk in iterable:\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 432, in _iterencode\n",
      "    yield from _iterencode_dict(o, _current_indent_level)\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 406, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 326, in _iterencode_list\n",
      "    yield from chunks\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 406, in _iterencode_dict\n",
      "    yield from chunks\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 439, in _iterencode\n",
      "    o = _default(o)\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/json/encoder.py\", line 180, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type float32 is not JSON serializable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration des chemins\n",
    "# -----------------------------\n",
    "DATA_DIR = \"/kaggle/input/dataset\"\n",
    "MODEL_DIR = \"/kaggle/working/models\"\n",
    "TEST_PATH = os.path.join(DATA_DIR, \"dataset_test_nodup.csv\")\n",
    "TOKEN_PATH = os.path.join(DATA_DIR, \"api_token_dict.csv\")\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"lstm_attention_model.keras\")\n",
    "\n",
    "# Alternative si le modÃ¨le .keras n'existe pas\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    MODEL_PATH = os.path.join(MODEL_DIR, \"lstm_attention_model.h5\")\n",
    "\n",
    "# -----------------------------\n",
    "# Classe pour tester les prÃ©dictions\n",
    "# -----------------------------\n",
    "class APIClassTester:\n",
    "    def __init__(self, model_path, test_data_path, token_path, maxlen=500):\n",
    "        self.maxlen = maxlen\n",
    "        self.model = None\n",
    "        self.test_df = None\n",
    "        self.token_df = None\n",
    "        self.labels = None\n",
    "        self.label_dict = None\n",
    "        self.reverse_label_dict = None\n",
    "        \n",
    "        self.load_data(test_data_path, token_path)\n",
    "        self.load_model(model_path)\n",
    "        self.results = []\n",
    "        \n",
    "    def load_data(self, test_data_path, token_path):\n",
    "        \"\"\"Charge les donnÃ©es de test et les tokens\"\"\"\n",
    "        print(\"ğŸ“ Chargement des donnÃ©es...\")\n",
    "        self.test_df = pd.read_csv(test_data_path)\n",
    "        self.token_df = pd.read_csv(token_path)\n",
    "        \n",
    "        # PrÃ©parer les labels\n",
    "        self.labels = sorted(self.test_df['label'].unique())\n",
    "        self.label_dict = {label: idx for idx, label in enumerate(self.labels)}\n",
    "        self.reverse_label_dict = {idx: label for label, idx in self.label_dict.items()}\n",
    "        \n",
    "        print(f\"âœ… DonnÃ©es chargÃ©es: {len(self.test_df)} Ã©chantillons\")\n",
    "        print(f\"ğŸ“‹ Classes dÃ©tectÃ©es: {self.labels}\")\n",
    "        \n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"Charge le modÃ¨le prÃ©-entraÃ®nÃ©\"\"\"\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"ğŸ¤– Chargement du modÃ¨le: {model_path}\")\n",
    "            self.model = load_model(model_path)\n",
    "            print(\"âœ… ModÃ¨le chargÃ© avec succÃ¨s!\")\n",
    "        else:\n",
    "            print(f\"âŒ ModÃ¨le introuvable: {model_path}\")\n",
    "            raise FileNotFoundError(f\"ModÃ¨le non trouvÃ©: {model_path}\")\n",
    "    \n",
    "    def preprocess_sequence(self, sequence_str):\n",
    "        \"\"\"PrÃ©process une sÃ©quence pour la prÃ©diction\"\"\"\n",
    "        try:\n",
    "            # Convertir la chaÃ®ne en liste\n",
    "            sequence = eval(sequence_str) if isinstance(sequence_str, str) else sequence_str\n",
    "            # Padding\n",
    "            padded = pad_sequences([sequence], maxlen=self.maxlen, padding='post', truncating='post')\n",
    "            return padded\n",
    "        except:\n",
    "            print(f\"âš ï¸ Erreur lors du preprocessing de: {sequence_str}\")\n",
    "            return None\n",
    "    \n",
    "    def predict_single(self, sequence_str):\n",
    "        \"\"\"Fait une prÃ©diction sur une sÃ©quence\"\"\"\n",
    "        processed_seq = self.preprocess_sequence(sequence_str)\n",
    "        if processed_seq is None:\n",
    "            return None, 0.0\n",
    "            \n",
    "        # PrÃ©diction\n",
    "        prediction = self.model.predict(processed_seq, verbose=0)\n",
    "        predicted_class_idx = np.argmax(prediction)\n",
    "        confidence = np.max(prediction)\n",
    "        \n",
    "        predicted_label = self.reverse_label_dict[predicted_class_idx]\n",
    "        return predicted_label, confidence\n",
    "    \n",
    "    def test_random_samples_per_class(self, samples_per_class=3):\n",
    "        \"\"\"Teste des Ã©chantillons alÃ©atoires pour chaque classe\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ§ª TEST DES PRÃ‰DICTIONS PAR CLASSE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        total_tests = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        for class_label in self.labels:\n",
    "            print(f\"\\nğŸ·ï¸  CLASSE: {class_label}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            # RÃ©cupÃ©rer les Ã©chantillons de cette classe\n",
    "            class_samples = self.test_df[self.test_df['label'] == class_label]\n",
    "            \n",
    "            if len(class_samples) == 0:\n",
    "                print(f\"âš ï¸ Aucun Ã©chantillon trouvÃ© pour la classe {class_label}\")\n",
    "                continue\n",
    "            \n",
    "            # SÃ©lectionner des Ã©chantillons alÃ©atoires\n",
    "            n_samples = min(samples_per_class, len(class_samples))\n",
    "            selected_samples = class_samples.sample(n=n_samples, random_state=42)\n",
    "            \n",
    "            class_correct = 0\n",
    "            \n",
    "            for idx, (_, row) in enumerate(selected_samples.iterrows(), 1):\n",
    "                sequence = row['sequence_tokenized']\n",
    "                true_label = row['label']\n",
    "                \n",
    "                # PrÃ©diction\n",
    "                predicted_label, confidence = self.predict_single(sequence)\n",
    "                \n",
    "                if predicted_label is None:\n",
    "                    print(f\"  {idx}. âŒ ERREUR DE PRÃ‰DICTION\")\n",
    "                    continue\n",
    "                \n",
    "                # VÃ©rifier si la prÃ©diction est correcte\n",
    "                is_correct = predicted_label == true_label\n",
    "                status_icon = \"âœ…\" if is_correct else \"âŒ\"\n",
    "                \n",
    "                print(f\"  {idx}. {status_icon} Vrai: {true_label} | PrÃ©dit: {predicted_label} | Confiance: {confidence:.3f}\")\n",
    "                \n",
    "                # Enregistrer les rÃ©sultats\n",
    "                self.results.append({\n",
    "                    'true_label': true_label,\n",
    "                    'predicted_label': predicted_label,\n",
    "                    'confidence': confidence,\n",
    "                    'is_correct': is_correct,\n",
    "                    'sequence_sample': str(sequence)[:100] + '...'  # Ã‰chantillon de sÃ©quence\n",
    "                })\n",
    "                \n",
    "                total_tests += 1\n",
    "                if is_correct:\n",
    "                    correct_predictions += 1\n",
    "                    class_correct += 1\n",
    "            \n",
    "            # Score pour cette classe\n",
    "            class_accuracy = (class_correct / n_samples) * 100 if n_samples > 0 else 0\n",
    "            print(f\"  ğŸ“Š Score classe {class_label}: {class_correct}/{n_samples} = {class_accuracy:.1f}%\")\n",
    "        \n",
    "        # Score global\n",
    "        overall_accuracy = (correct_predictions / total_tests) * 100 if total_tests > 0 else 0\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ“Š RÃ‰SULTATS FINAUX\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"ğŸ¯ Score global: {correct_predictions}/{total_tests} = {overall_accuracy:.2f}%\")\n",
    "        print(f\"ğŸ“ˆ PrÃ©dictions correctes: {correct_predictions}\")\n",
    "        print(f\"âŒ PrÃ©dictions incorrectes: {total_tests - correct_predictions}\")\n",
    "        \n",
    "        return overall_accuracy, self.results\n",
    "    \n",
    "    def detailed_analysis(self):\n",
    "        \"\"\"Analyse dÃ©taillÃ©e des rÃ©sultats par classe\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"âš ï¸ Aucun rÃ©sultat Ã  analyser\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ“ˆ ANALYSE DÃ‰TAILLÃ‰E PAR CLASSE\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Grouper par classe rÃ©elle\n",
    "        class_stats = defaultdict(lambda: {'total': 0, 'correct': 0, 'confidences': []})\n",
    "        \n",
    "        for result in self.results:\n",
    "            true_label = result['true_label']\n",
    "            class_stats[true_label]['total'] += 1\n",
    "            class_stats[true_label]['confidences'].append(result['confidence'])\n",
    "            if result['is_correct']:\n",
    "                class_stats[true_label]['correct'] += 1\n",
    "        \n",
    "        # Afficher les statistiques\n",
    "        for class_label in sorted(class_stats.keys()):\n",
    "            stats = class_stats[class_label]\n",
    "            accuracy = (stats['correct'] / stats['total']) * 100\n",
    "            avg_confidence = np.mean(stats['confidences'])\n",
    "            \n",
    "            print(f\"ğŸ·ï¸  {class_label}:\")\n",
    "            print(f\"    PrÃ©cision: {stats['correct']}/{stats['total']} = {accuracy:.1f}%\")\n",
    "            print(f\"    Confiance moyenne: {avg_confidence:.3f}\")\n",
    "            print()\n",
    "    \n",
    "    def save_results(self, output_path=\"/kaggle/working/test_results.json\"):\n",
    "        \"\"\"Sauvegarde les rÃ©sultats au format JSON\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"âš ï¸ Aucun rÃ©sultat Ã  sauvegarder\")\n",
    "            return\n",
    "        \n",
    "        # PrÃ©parer les donnÃ©es pour la sauvegarde\n",
    "        save_data = {\n",
    "            'metadata': {\n",
    "                'total_tests': len(self.results),\n",
    "                'correct_predictions': sum(1 for r in self.results if r['is_correct']),\n",
    "                'overall_accuracy': sum(1 for r in self.results if r['is_correct']) / len(self.results) * 100,\n",
    "                'classes_tested': list(set(r['true_label'] for r in self.results))\n",
    "            },\n",
    "            'results': self.results\n",
    "        }\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"ğŸ’¾ RÃ©sultats sauvegardÃ©s: {output_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Script principal\n",
    "# -----------------------------\n",
    "def main():\n",
    "    try:\n",
    "        # Initialiser le testeur\n",
    "        tester = APIClassTester(\n",
    "            model_path=MODEL_PATH,\n",
    "            test_data_path=TEST_PATH,\n",
    "            token_path=TOKEN_PATH,\n",
    "            maxlen=500\n",
    "        )\n",
    "        \n",
    "        # Tester 2-3 Ã©chantillons par classe\n",
    "        print(\"ğŸš€ Lancement des tests...\")\n",
    "        overall_accuracy, results = tester.test_random_samples_per_class(samples_per_class=3)\n",
    "        \n",
    "        # Analyse dÃ©taillÃ©e\n",
    "        tester.detailed_analysis()\n",
    "        \n",
    "        # Sauvegarder les rÃ©sultats\n",
    "        tester.save_results()\n",
    "        \n",
    "        print(f\"\\nğŸ‰ Tests terminÃ©s! Score final: {overall_accuracy:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur lors des tests: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# -----------------------------\n",
    "# Fonction de test individuel (pour usage manuel)\n",
    "# -----------------------------\n",
    "def test_individual_prediction(sequence_tokenized, true_label=\"unknown\"):\n",
    "    \"\"\"\n",
    "    Fonction pour tester une prÃ©diction individuelle\n",
    "    Usage: test_individual_prediction(\"[1, 2, 3, 4, ...]\", \"vraie_classe\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tester = APIClassTester(MODEL_PATH, TEST_PATH, TOKEN_PATH)\n",
    "        predicted_label, confidence = tester.predict_single(sequence_tokenized)\n",
    "        \n",
    "        print(f\"ğŸ·ï¸  Vraie classe: {true_label}\")\n",
    "        print(f\"ğŸ¤– PrÃ©diction: {predicted_label}\")\n",
    "        print(f\"ğŸ“Š Confiance: {confidence:.3f}\")\n",
    "        print(f\"âœ… Correct: {'Oui' if predicted_label == true_label else 'Non'}\")\n",
    "        \n",
    "        return predicted_label, confidence\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur: {e}\")\n",
    "        return None, 0.0\n",
    "\n",
    "# ExÃ©cuter le script principal\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8283601,
     "sourceId": 13079061,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 939.370478,
   "end_time": "2025-09-16T17:50:28.908730",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-16T17:34:49.538252",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
